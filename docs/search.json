[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "data_management",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "chap2.html",
    "href": "chap2.html",
    "title": "data_management",
    "section": "",
    "text": "Data management is important in the research life cycle.\nA data management plan (DMP) is a formal document that outlines data management strategies during and after a research project.\nMany funding agencies require grantees to submit a DMP as part of the proposal package.\nWriting a comprehensive DMP can encourage researchers to think carefully about data management needs.\nThe transparency and openness of publicly funded research data requires it to be discoverable, accessible, and reusable to the public.\nA well-managed data plan can provide a greater return on investment and benefits for verification, reduction of scientific fraud, promotion of new research, resources for training new researchers, and discouraging unintentional redundancy in research.\n\n\n\n\n\nUS funding agencies have data management policies\nNational Science Foundation (NSF) requires a Data Management Plan (DMP) document of up to 2 pages, covering 5 key aspects:\n\nTypes of data produced\nData and metadata format and content\nPolicies for access and sharing, including security provisions\nPolicies and provisions for dissemination and reuse\nPlans for archiving, preserving, and providing access to the data\n\nNational Institutes of Health (NIH) requires a data sharing plan for applicants seeking $500,000 or more, which should include:\n\nFinal dataset formats\nDocumentation\nAnalytic tools necessary to use the data\nData sharing agreements\nHow and when the data will be made accessible to others\n\nNational Endowment for the Humanities (NEH) requires a DMP that answers two questions:\n\nWhat data are generated by the research?\nWhat is the plan for managing these data?\n\nNEH requires the DMP to describe:\n\nThe data to be produced\nHow the data will be managed and maintained\nAny factors that might impact the researcher’s ability to manage the data\nTypes of information that need to be managed and maintained alongside the data\nHow data will be shared in a timely manner\nThe facilities used to preserve the data\n\n\n\n\n\n\nA data management plan (DMP) should describe all major aspects of data management throughout the research life cycle.\nThe Digital Curation Center provides a checklist for a data management plan with questions that should be addressed in the DMP.\nThe checklist includes questions such as: what data will you collect or create? How will the data be collected or created? What metadata documentation will accompany the data? How will you manage ethical and legal issues related to data ownership, privacy, and copyright? How will you store and backup the data? How will you manage access and security? Which data should be retained, shared, and/or preserved? What is the long-term preservation plan for the dataset? How will you share the data? Who will be responsible for data management? What resources will you require to implement your plan?\nThe DMP should justify the data format choice and include storage implications due to the format or volume of the data.\nThe DMP should describe the data collection methods, organizing data files, applying version control, and implementing quality assurance protocols.\nThe DMP should identify provisions for protecting the confidentiality of human participants if the project involves research with human subjects.\nThe DMP should identify the owner of the data and discuss permissions with the data producer if third-party data is being reused.\nThe DMP should include provisions for systematic backups of the data files and describe security measures in detail.\nThe DMP should identify the repository to be used to archive the data and plans to prepare and document the data for long-term preservation.\nThe DMP should identify the mechanism for sharing the data and any restrictions required for data sharing.\nThe DMP should identify who will oversee the implementation of the data management plan and what resources will be required for data management tasks and long-term preservation.\n\n\n\n\n\nMultiple data management plan tools available online for free\nTwo popular tools are the DMPTool and DMPOnline\nDMPTool provides customized data management plans based on funding agency requirements\nDMPTool offers office resources, help text, and suggested answers based on templates developed and reviewed by information professionals\nDMPTool is regularly updated to include new funding agency policies\nDMPOnline is the data management planning tool developed by the Digital Curation Centre (DCC)\nDMPOnline offers guidance and templates specific to UK and non-UK funding agencies\nInstitutions can customize templates via DMPOnline\nWriting a data management plan helps researchers consider data management issues early on in the research life cycle\nWriting a data management plan leads to better compliance with data management requirements and better preparation for implementing data management strategies\nWriting a data management plan is a helpful process for better research, even if not required."
  },
  {
    "objectID": "sumary.html",
    "href": "sumary.html",
    "title": "data_management",
    "section": "",
    "text": "Summary & Additional Resources\nOverview This lesson covered multiple definitions of research data and examined the varying types of research data including associated researcher materials that need to be included alongside data to ensure they remain meaningful. It also introduced learners to two key concepts examined in this course: metadata and the research data lifecycle. Understanding the heterogeneous nature of data can help researchers and information professionals appreciate how different disciplines conceptualize data. Building a familiarity with basic data management concepts will also enable researchers and information professionals to share a common vocabulary when discussing data management.\nKey Concepts & Definitions Research data can take on many different forms, and definitions of research data can vary based on the research community.\nThe National Institutes of Health define research data as “Recorded factual material commonly accepted in the scientific community as necessary to document and support research findings” (National Institutes of Health).\nThe National Science Foundation’s definition describes data as something “determined by the community of interest through the process of peer review and program management” (National Science Foundation). Examples of data provided by NSF include data, publications, samples, physical collections, software, and models.\nThe National Endowment for the Humanities defines data as “materials generated or collected during the course of conducting research” and provides a wider variety of examples of data than NIH or NSF (National Endowment for the Humanities).\nKey take-away: Data should be valid, shared, and are heterogeneous and contextualized within research communities.\nThere are other important research products that may need to be provided alongside the data in order for the data to be meaningful and to support secondary analysis. These may include codebooks, questionnaires, descriptions of methodologies, reports, conference posters, articles, white papers, websites or blogs, etc.\nMetadata is defined as structured information that describes, explains, locates, and otherwise represents something else. Metadata allows data to be found and interpreted. At a minimum, one needs to know who created the data, when the data were created or published, and a title or descriptive name used to refer to the dataset. Digital data should also have a unique and persistent identifier. Two metadata standards commonly used to describe research data are Dublin Core and the Data Documentation Initiative.\nKey take-away: Without supporting documentation and metadata, data may be rendered meaningless and unusable.\nThe research data lifecycle can be depicted in multiple different ways from simple to complex. It provides a useful way to think through the various different stages that data go through during a research project. Key phases of the research data lifecycle created by the Data Documentation Initiative include: Discovery and Planning, Initial Data Collection, Data Preparation and Analysis, Publication and Sharing, and Long-term Management.\nKey take-away: Proper data management takes place across the entire research data lifecycle!\nReferences This list includes the literature, resources, and other visual content that appear in this lesson.\nDublin Core Metadata Initiative. (2012). DCMI metadata terms. Retrieved from http://dublincore.org/documents/dcmi-terms/#elements-contributor\nDDI Alliance. (2015). Welcome to the Data Documentation Initiative. Retrieved from http://www.ddialliance.org/\nGilles, C. (2010). Metadata [Online Image]. Retrieved from https://flic.kr/p/8zcfLD [CC BY 2.0]\ngnizr. (2007). Database schema [Online Image]. Retrieved from https://flic.kr/p/4dyNMq [CC BY 2.0]\nKrebelj, M. (2009). HTML code [Online Image]. Retrieved from https://flic.kr/p/6WxYTc [CC BY 2.0]\nNASA Goddard Space Flight. (2014). NASA’s Ship-Aircraft Bio-Optical Research (SABOR) [Online Image]. Retrieved from https://flic.kr/p/oStHsL [CC BY 2.0]\nNational Information Standards Organization (U.S.). (2004). Understanding metadata. Bethesda, MD: NISO Press. Retrieved from http://www.niso.org/standards/resources/UnderstandingMetadata.pdf‎\nNational Institutes of Health (NIH). (2003). NIH data sharing policy. Bethesda, MD: National Institutes of Health. Retrieved from http://grants.nih.gov/grants/policy/data_sharing/\nNational Endowment for the Humanities (NEH). (2015). Data management plans for NEH office of digital humanities proposals and awards. Washington, DC: National Endowment for the Humanities. Retrieved from http://www.neh.gov/files/grants/data_management_plans_2015.pdf\nNational Science Foundation (NSF). (2010). Data management & sharing frequently asked questions (FAQs). Arlington, VA: National Science Foundation. Retrieved from https://www.nsf.gov/bfa/dias/policy/dmpfaqs.jsp#1\nRagan, S. M. (2010). Life size lego syringe - blood - trans red [Online Image]. Retrieved from https://flic.kr/p/8XEkRf [CC BY 2.0 https://creativecommons.org/licenses/by/2.0/]\nRussell, J. (2005). Hospital corpsmen verify the result of a blood sample [Online Image]. Retrieved from https://goo.gl/KB5Y6i [Public Domain]\nStansberry, B. (2009). Cove-mtn-air-quality-station-tn1.jgp [Online Image]. Retrieved from https://commons.wikimedia.org/wiki/File:Cove-mtn-air-quality-station-tn1.jpg [CC BY 3.0]\nUK Data Archive, University of Essex. (n.d.). Research data lifecycle [Image]. Retrieved from http://www.data-archive.ac.uk/create-manage/life-cycle\nUK Digital Curation Centre. (n.d.). DCC curation lifecycle model [Image]. Retrieved from http://www.dcc.ac.uk/resources/curation-lifecycle-model\nUniversity of Virginia Library. (2014). Steps in the data life cycle [Image]. Retrieved from http://dmconsult.library.Virginia.edu/lifecycle\nWallis, J. C., Rolando, E., & Borgman, C. L. (2013). If we share data, will anyone use them? Data sharing and reuse in the long tail of science and technology. PLoS ONE, 8(7), e67332. http://doi.org/10.1371/journal.pone.0067332\nAll lecture videos include the song “Air Hockey Saloon” by Chris Zabriskie licensed under CC BY 4.0.\nAdditional Resources Australian National Data Service. (August 2011). What is research data? Retrieved from http://ands.org.au/guides/what-is-research-data\nHey, T., Tansley, S., Tolle K. (2009). The fourth paradigm: Data-intensive scientific discovery. Retrieved from research.microsoft.com/en-us/collaboration/fourthparadigm/4th_paradigm_book_complete_lr.pdf\nNature (Editor). (2009). Data - what data? Nature, 461(7261), 145-304. Retrieved from http://www.nature.com/nature/journal/v461/n7261/\n\n\nSummary & Additional Resources\nOverview This lesson defined the concept of “data management”, explained why data management is an important part of the responsible conduct of research, identified key stakeholders in data management and their roles and responsibilities, and illustrated how data management fits within the research lifecycle.\nKey Concepts & Definitions Data management refers to the everyday handling and workflow of research data during the active phase of a project as well as the practices that support long-term preservation, access, and use after the project has been completed. These activities can include planning, documenting data, formatting data, storing data, anonymizing data, and controlling access to data.\nData Management, as defined by NOAA’s Administrative Order 212-15, “consists of two major activities conducted in coordination: data management services and data stewardship. They constitute a comprehensive end-to-end process including movement of data and information from the observing system sensors to the data user. This process includes the acquisition, quality control, metadata cataloging, validation, reprocessing, storage, retrieval, dissemination, and archival of data.”\nThere are numerous reasons why data management is important. It makes it possible for other researchers to discover, interpret, and reuse data. It helps sustain the value of data by enabling others to verify and build upon published results. It facilitates long-term preservation of and access to datasets. Finally, a growing number of funding agencies, journal publishers, and research institutions require it.\nKey take-away: Data management is an integral part of the responsible conduct of research. It helps researchers optimize use of data during the active phase of the research project and helps them collaborate with other researchers.\nData management stakeholders may vary depending on the setting and subject domain but often include the following:\nPrimary Researcher or Principal Investigator: Creates and uses data\nInstitution: Sets internal data management policy\nData Repository: Curates and provides access to data\nUser: Uses 3rd party data\nFunder: Provides the resources to support a research project\nPublisher: Disseminates discoveries and maintains the scientific record\nKey take-away: The variety of stakeholders suggests that proper management of data throughout the research lifecycle requires communication and cooperation between these various groups.\nData Management Across the Research Lifecycle Discovery & Planning: This is the stage to determine if a project will produce a new dataset, combine existing datasets or analyze existing datasets; identify privacy, confidentiality, and other ethical issues; consider documentation format and content and the metadata standards to use to describe the data; identify potential users of project data; identify an appropriate data repository to archive the data; and determine data management costs. These details should be included in the data management plan.\nInitial Data Collection: This is the stage during which to determine workflows and procedures for organizing files, backups and storage, performing quality assurance protocols, and setting appropriate access controls and security measures.\nPreparation & Data Analysis: Researchers may need to clean, manipulate, or process the raw data. What is important during this stage is to document the changes to the raw data and creating a “master” version to be analyzed and eventually archived. It is also essential to document analysis procedures such as additional modifications to the data, the model used, the code used to run the analysis, and hardware and software specifications.\nPublication and Sharing: This is the stage to consult with the archive or repository to determine file formats, clean and further document data. Additionally, all documentation should be reviewed to ensure that the they contain enough information to allow the data to be re-used by a third party.\nLong-term Management: This is the stage during which researchers share their data and findings through publications, submit reports, and deposit data and supplementary materials into the archive or repository.\nKey take-away: Effective data management takes place during all phases of the research lifecycle from planning your project to collecting your data to preparing and analyzing your data and then finally publishing and sharing your data in a repository that will see to their long term management.\nReferences This list includes the literature, resources, and other visual content that appear in this lesson.\nArchaeology Data Service. (2015). Archaeology Data Service: Archives [Image, Screen capture]. Retrieved from http://archaeologydataservice.ac.uk/archives/\nData Archiving and Networked Services. (2015). DANS - Home [Image, Screen capture]. Retrieved from http://www.dans.knaw.nl/en\nDDI Alliance. (2008). Welcome to the Data Documentation Initiative. Retrieved from http://www.ddialliance.org/\nDRYAD. (2015). DRYAD Digital Repository [Image, Screen capture]. Retrieved from http://datadryad.org/\nEconomic and Social Research Council. (2015). Home - Economic and Social Research Council [Image, Screen capture]. Retrieved from http://www.esrc.ac.uk/\nGESIS: Leibniz Institute for the Social Sciences. (2015). GESIS: Research [Image, Screen capture]. Retrieved from http://www.gesis.org/forschung/leibniz-forschungsverbuende/\nInstitute for Quantitative Social Science. (2015). Harvard Dataverse [Image, Screen capture]. Retrieved from https://dataverse.harvard.edu/\nInter-university Consortium for Political and Social Research (ICPSR). (2012). Guide to social science data preparation and archiving: Best practice throughout the data life cycle (5th ed.). Ann Arbor, MI: ICPSR. Retrieved from http://www.icpsr.umich.edu/files/ICPSR/access/dataprep.pdf\nLyon, L. (2007). Dealing with data: Roles, rights, responsibilities, and relationships [Consultancy Report]. UK: UKOLN, University of Bath. Retrieved from http://www.ukoln.ac.uk/ukoln/staff/e.j.lyon/reports/dealing_with_data_report-final.p\nNational Center for Biotechnology Information - dbGaP. (2015). Home - dbGaP [Image, Screen capture]. Retrieved from http://www.ncbi.nlm.nih.gov/gap\nNational Endowment for the Humanities. (2015). NEH - National Endowment for the Humanities [Image, Screen capture]. Retrieved from http://www.neh.gov/\nNational Institutes of Health. (2015). National Institutes of Health (NIH) [Image, Screen capture]. Retrieved from http://www.nih.gov/\nNational Oceanic and Atmospheric Administration. (2010). NAO 212-15: Management of environmental data and information. National Oceanic and Atmospheric Administration. Retrieved from http://www.corporateservices.noaa.gov/ames/administrative_orders/chapter_212/212-15.html\nNational Science Foundation. (2015). NSF - National Science Foundation [Image, Screen capture]. Retrieved from http://www.nsf.gov/\nUK Data Archive. (2015). UK Data Archive: Deposit data [Image, Screen capture]. Retrieved from http://www.data-archive.ac.uk/deposit\nAll lecture videos include the song “Air Hockey Saloon” by Chris Zabriskie licensed under CC BY 4.0.\nAdditional Resources Lynch, C. (2014). The next generation of challenges in the curation of scholarly data. In J. M. Ray (Ed.), Research data management: Practical strategies for information professionals. West Lafayette, Indiana: Purdue University Press (pp. 395-408). Retrieved from http://www.cni.org/wp-content/uploads/2013/10/Research-Data-Mgt-Ch19-Lynch-Oct-29-2013.pdf\nStrasser, C., Cook, R., Michener, W. & Budden, A. (2012). DataONE best practices primer on data management: What you always wanted to know but were afraid to ask. Retrieved from https://www.dataone.org/sites/all/documents/DataONE_BP_Primer_020212.pdf\nStrasser, C. (2015). Research data management: A primer. NISO Press: Baltimore, MD. Retrieved from http://www.niso.org/apps/group_public/download.php/15375/PrimerRDM-2015-0727.pdf\nSurkis, A. and Read, K. (2015, July). Research data management. Journal of the Medical Library Association, 103(3), 154–156. Retrieved from http://dx.doi.org/10.3163/1536-5050.103.3.011\nUniversity of Bristol. (2013). Research data management glossary. Retrieved from http://vocab.bris.ac.uk/data/glossary/\nUniversity of Edinburgh. (n.d.). Research data management. Retrieved from http://www.ed.ac.uk/files/imports/fileManager/ResearchDataManagement.pdf\nWhyte, A., Tedds, J. (2011). Making the case for research data management. DCC Briefing Papers. Edinburgh: Digital Curation Centre. Retrieved from http://www.dcc.ac.uk/resources/briefing-papers#sthash.E0hMa63k.dpuf\n\n\nSummary & Additional Resources\nOverview This lesson provided a basic overview of Data Management Plans. Most funding agencies now require grantees to submit Data Management Plans as a part of the grant proposal package. Therefore, it is important for researchers as well as information professionals supporting researchers to understand how to create a DMP. This lesson also discussed available DMP tools that can assist researchers and information professionals in the development of comprehensive DMPs.\nKey Concepts & Definitions Data Management Plans (otherwise known as DMPs) are formal documents that describe the data produced during a research project and outline data management strategies that will be implemented during and after the active phase of the research project. DMPs also describe in detail all aspects of data management that will take place during the entire research data lifecycle when data are being collected, organized, documented, shared, and preserved.\nA growing number of funding agencies including the National Science Foundation, the National Institutes of Health, the National Endowment for the Humanities, the Economic and Social Research Council, and the Wellcome Trust require researchers to submit Data Management Plans. Funding agencies require grantees to submit Data Management Plans because they provide assurances that data will be accessible and usable over the long term. This is important for two primary reasons:\nData Management Plans support transparency and openness. Data management plans that illustrate how data will be made discoverable, accessible, and reusable support the responsibility to make publicly funded research products openly available.\nData Management Plans support return on investment. Making data discoverable, accessible, and reusable maximizes the research potential of the data and provides greater returns on public investments in research.\nKey take-away: Today, DMPs are required by a growing number of funding agencies. Two key reasons why funders require DMPs is to support transparency and openness and their return on investment.\nThe Digital Curation Centre’s Checklist for a Data Management Plan provides a useful list of questions to consider when writing a DMP:\nWhat data will you collect or create?\nHow will the data be collected or created?\nWhat documentation and metadata will accompany the data?\nHow will you manage any ethical issues?\nHow will you manage copyright and intellectual property rights issues?\nHow will the data be stored and backed up during research?\nHow will you manage access and security?\nWhich data should be retained, shared, and/or preserved?\nWhat is the long-term preservation plan for the dataset?\nHow will you share the data?\nAre any restrictions on data sharing required?\nWho will be responsible for data management?\nWhat resources will you require to implement your plan?\nKey take-away: Writing a DMP helps researchers think through data management issues early in the research lifecycle, which can help researchers “do better research” and ensure that data will remain accessible and usable over the long-term.\nTwo helpful (and free!) data management planning tools available to researchers and information professionals include the DMPTool, created by the University of California Curation Center of the California Digital Library, and DMPOnline, developed by the UK Digital Curation Centre. Both of these tools provide guidance and templates for creating Data Management Plans in compliance with institutional and funder requirements. For more information on these tools check out the following videos:\nDMPTool Overview Video: https://dmptool.org/video\nDMPOnline Video: http://www.screenr.com/PJHN\nKey take-away: You are not alone! There are numerous free, online tools available to help researchers and information professionals create Data Management Plans.\nReferences This list includes the literature, resources, and other visual content that appear in this lesson.\nDMPOnline. (n.d.). About DMPOnline. Retrieved from https://dmponline.dcc.ac.uk/about_us\nDMPTool. (n.d.). About the DMPTool. Retrieved from https://dmptool.org/about\nEconomic and Social Research Council. (2015). Home - Economic and Social Research Council [Image, Screen capture]. Retrieved from http://www.esrc.ac.uk/\nKnight, G. (2012). A crowded mall [Online Image]. Retrieved from https://flic.kr/p/bVQKzX [CC BY 2.0]\nNational Endowment for the Humanities (NEH). (2015). Data management plans for NEH office of digital humanities proposals and awards. Washington, DC: National Endowment for the Humanities. Retrieved from http://www.neh.gov/files/grants/data_management_plans_2015.pdf\nNational Endowment for the Humanities (NEH). (2015). NEH - National Endowment for the Humanities [Image, Screen capture]. Retrieved from http://www.neh.gov/\nNational Institutes of Health (NIH). (2003). Final NIH statement on sharing research data (Notice No. NOT-OD-03-032). Bethesda, MD: National Institutes of Health. Retrieved from http://grants.nih.gov/grants/guide/notice-files/NOT-OD-03-032.html\nNational Institutes of Health (NIH). (2015). National Institutes of Health (NIH) [Image, Screen capture]. Retrieved from http://www.nih.gov/\nNational Science Foundation (NSF). (2010). Dissemination and sharing of research results. Arlington, VA: National Science Foundation. Retrieved from https://www.nsf.gov/bfa/dias/policy/dmpfaqs.jsp#1\nNational Science Foundation (NSF). (2015). NSF - National Science Foundation [Image, Screen capture]. Retrieved from http://www.nsf.gov/\nQuintano, A. (2014). Sunrise in Washington D.C. 2014 [Online Image]. Retrieved from https://flic.kr/p/nzJx4W [CC BY 2.0]\nUK Digital Curation Centre. (2014). Checklist for a data management plan (v.4.0). Retrieved from http://www.dcc.ac.uk/sites/default/files/documents/resource/DMP/DMP_Checklist_2013.pdf\nAll lecture videos include the song “Air Hockey Saloon” by Chris Zabriskie licensed under CC BY 4.0.\nAdditional Resources Digital Curation Centre (DCC). (2010). Data management plans. Retrieved from www.dcc.ac.uk/resources/data-management-plans\nOffice of Science and Technology Policy. (2013). Increasing access to the results of federally funded scientific research memorandum. Washington, D.C.: Executive Office of the President. Retrieved from http://www.whitehouse.gov/sites/default/files/microsites/ostp/ostp_public_access_memo_2013.pdf\nUK Data Archive. (n.d.). Create and manage data: planning for sharing. Retrieved from www.data-archive.ac.uk/create-manage/planning-for-sharing\n\n\nSummary & Additional Resources\nOverview In this lesson (module) you were introduced to the concepts of research data organisation or housekeeping: what constitutes good data file management; the importance of managing data; file versioning, naming and re-naming conventions.\nKey Concepts & Definitions Research data files and folders need to be labelled and organised in a systematic way so that they are both identifiable and accessible for current and future users.\nNaming files according to agreed conventions should make file naming easier for you as well as for others because there will be no need to ‘re-think’ the process each time.\nAnother benefit of consistent research data file labelling is that files are not accidentally overwritten or deleted.\nIt is important to consistently identify and distinguish versions of research data files as well. This ensures that a clear audit trail exists for tracking the development of a data file and identifying earlier versions when needed.\nKey take-away: Whatever you do, be consistent throughout the process.\nAdditional Resources Frazer, M. (14 January 2013). An Elevator Pitch for File Naming Conventions. ACRL TechConnect blog. Retrieved from http://acrl.ala.org/techconnect/?p=2607\nJISC Digital Media. Choosing a File Name [Managing]. Jisc. Retrieved 4 June 2014 from\nhttp://www.webarchive.org.uk/wayback/archive/20160101151739/http:/www.jiscdigitalmedia.ac.uk/guide/choosing-a-file-name\nMIT Libraries. Data management and publishing: Organize your files. Massachusetts Institute of Technology. Retrieved from http://libraries.mit.edu/data-management/store/organize\n\n\nSummary & Additional Resources\nOverview In this module you were introduced to concepts of data file formatting, compression, normalisation, and other kinds of data transformation and why they are useful and important.\nKey Concepts & Definitions If you need to convert or migrate your data files from one format to another, you need to be aware of the potential risk of the loss or corruption of your data and take appropriate steps to avoid/minimise it.\nWhen compressing your data files for the purpose of storage, transportation or transmission, you encode the information using fewer bits than the original representation. Commonly used compression programs are, zip, GNU Zip (.gzip or .tar.gz) and Stuffit.\nIn your research you may also use the process of data normalisation; two meanings of this that may be of relevance to you are statistical normalisation and database normalisation.\nYou may also need to compute new values from old in your data, a process which is called data transformation, which may also be a necessary prelude to analysing your data. Three techniques which could be classified as data transformation are aggregation (combining data into larger units), anonymization (removing information identifying human subjects) and perturbation (distortion).\nKey take-away: If you are saving files with proprietary formats for the long-term, consider creating a plain text or open format version to save along with them.\nAdditional Resources Arms, C. R, Fleischhauer, C., Murray, K. (2013). ‘Sustainability Factors’ in Sustainability of Digital Formats: Planning of Library of Congress Collections. Library of Congress. Compilation. Last updated 20 March 2013. Retrieved from www.digitalpreservation.gov/formats/sustain/sustain.shtml\nDBnormalization.com. Database Design Basics: Introduction. Last viewed 22 March 2016. Retrieved from http://www.dbnormalization.com/\nEtzkorn, B. (6 November 2011). Data normalization and standardization. Retrieved from www.benetzkorn.com/2011/11/data-normalization-and-standardization\nFrancois, D. (29 January 2010). Data normalization for statistical analysis. Retrieved from www.damienfrancois.be/blog/pivot/entry.php?id=8\nState Archives of North Carolina. (2012). File format guidelines for management and long-term retention of electronic records. Retrieved from http://archives.ncdcr.gov/Portals/3/PDF/guidelines/file_formats_in-house_preservation.pdf?ver=2016-03-11-084033-067\n\n\nSummary & Additional Resources\nOverview In this lesson you were introduced to the importance of documenting your research data, and why and how to cite data.\nKey Concepts & Definitions There are many reasons why you need to document your data:\nto help you remember the details later\nto help others understand your research, verify your findings, review your submitted publication, replicate your results, or even archive your data for access and re-use.\nResearch data need to be documented at various levels:\nproject level\nfile or database level\nand variable or item level.\nSome examples of data documentation are:\nlaboratory notebooks\nmethodology reports\nquestionnaires\nand software syntax.\nLaboratory notebooks, for example, play an important role in supporting claims relating to intellectual property developed by researchers, and even defending against claims of scientific fraud.\nData citation promotes the reproduction of research results, allows the impact and use of data to be tracked, and provides a structure which recognises and rewards the data creator.\nKey take-away: By providing good documentation and a citation for your work, you make it easier for others to re-use your data, cite your work, and give you acknowledgement in the scholarly record. When re-using others’ data you should give it a proper citation as well.\nAdditional Resources Ball, A., Duke, M. (18 October 2012). How to cite datasets and link to publications [How-to guides]. Digital Curation Centre. Last updated 20 June 2012. Retrieved from www.dcc.ac.uk/resources/how-guides/cite-datasets\nCODATA-ICSTI Task Group on Data Citation Standards and Practices (2013). ‘Out of cite, out of mind: The current state of practice, policy, and technology for the citation of data’ in Data Science Journal. Vol. 12, p. CIDCR1-CIDCR75. Retrieved from dx.doi.org/10.2481/dsj.OSOM13-043\nData Citation Synthesis Group (2014). Joint Declaration of Data Citation Principles. Martone M. (ed.) San Diego CA: FORCE11. Retrieved from www.force11.org/group/joint-declaration-data-citation-principles-final\nDataCite. (2009). Why cite data? Retrieved from www.datacite.org/whycitedata\nUK Data Archive. Create and manage data: documenting your data. Retrieved from http://www.data-archive.ac.uk/create-manage/document\nUK Data Service. Citing data. Retrieved from ukdataservice.ac.uk/use-data/citing-data.aspx\n\n\nSummary & Additional Resources\nOverview In this lesson you were introduced to issues involved in storing, securing and backing up your research data, the importance of data backups, options available to you to safely store your data, password safety guidelines, and how to encrypt and destroy sensitive data when required.\nKey Concepts & Definitions In this module you were introduced to issues involved in storing, securing and backing up your research data, the importance of data backups, options available to you to safely store your data, password safety guidelines, and how to encrypt and destroy sensitive data when required.\nThroughout the course of your research you must ensure that you store your research data in a secure way and have backup copies in at least three locations that are maintained regularly. The recommended storage is your institution’s networked drive. Personal computers and laptops should not be used for storing master copies of data, and external storage devices are not recommended for the long term storage of data, particularly master copies.\nSecuring research data is an important aspect of information technology security. You should always have up-to-date anti-virus software installed on your office and home computers, encrypt highly-sensitive electronic data and store paper records in a locked room or safe.\nOnce research using sensitive data no longer requires that the identifiable portions of the data be retained, they should be destroyed and future research be done with de-identified or anonymised data. This applies to paper records, as well as electronic records.\nRemember that your passwords are the most common way to prove your identity when using websites, email accounts and your computer itself (via User Accounts). Therefore, always use a strong password, never disclose your password to anyone else. Make your password something you can remember but difficult for others to guess.\nKey take-away: Don’t be the next sob story about data loss. Take precautions up front so you will not have regrets later. Know how your data are backed up.\nAdditional Resources Digital Preservation Toolkit. (Last updated Nov 2015). Government of Canada. Retrieved from http://canada.pch.gc.ca/eng/1443203443442\nFinch, L., Webster, J. (July 2008). Caring for CDs and DVDs. National Preservation Office. NPO Preservation Guidance [Preservation in Practice Series]. Retrieved from http://www.bl.uk/aboutus/stratpolprog/collectioncare/faqs/cddvd/caring_for_cds_dvds.pdf\nUK Data Service. Create and manage data: storing your data. Retrieved from www.data-archive.ac.uk/create-manage/storage\nWikipedia. Data erasure. Retrieved from en.wikipedia.org/wiki/Data_erasure"
  },
  {
    "objectID": "chap1.html",
    "href": "chap1.html",
    "title": "data_management",
    "section": "",
    "text": "The fundamental question of the course is what are data, and different organizations have tackled it resulting in various definitions.\nData are different for various disciplines and contexts, and there are multiple types of data in an array of contexts, including numeric and textual data, biological samples, and physical collections.\nResearch data and associated researcher materials should be distinguished, and some of them may be required alongside the data to understand them.\nThe most important concept in the entire course is understanding data in the context of the research data life cycle, from project planning to archiving that data as a research output after the project period has ended.\nThe National Institutes of Health define data as recorded factual material necessary to validate research findings, and the National Science Foundation considers data determined by the community of interest through the process of peer review and project management.\nThe NSF examples of data include data, publications, samples, physical collections, software, and models, while the NEH examples include citations, software code, algorithms, digital tools, documentation, databases, geospatial coordinates, reports, and articles.\nKey concepts in these definitions include validity, data sharing among the community, heterogeneity, and contextualization within research communities.\n\n\n\n\n\nTypes of data include numeric/tabular data, samples (e.g. DNA, blood), physical collections (e.g. plant specimens), software programs/code, databases, algorithms, models, and geodatabases.\nBackground data provides contextual information for analysis and includes questionnaires, code books, and descriptions of methodologies.\nResearch products built on data include reports, conference posters, articles, white papers, books, websites, and blogs.\nMetadata is structured information that describes, explains, locates, or represents something else (in this case, research data).\nMinimum metadata elements include who created the data, when it was created/published, and a descriptive name for the dataset. A unique identifier is also necessary to locate the data.\nThe Dublin Core Metadata Element Set and Data Documentation Initiative (DDI) provide additional metadata elements for finding, identifying, interpreting, and using data.\n\n\n\n\n\nProper data management is crucial throughout the research lifecycle.\nEach stage of the lifecycle requires different considerations, responsibilities, and activities.\nThere are different lifecycle models, such as the UK Data Archive, the University of Virginia library, and the Digital Curation Center.\nThe Digital Curation Center’s model includes various data curation activities and involves data, researchers, and curators.\nIntroductions and discussions about personal data experiences or questions are encouraged in the forums.\nAdditional readings and resources are available for those interested in learning more."
  },
  {
    "objectID": "chap1.html#understanding-data-management",
    "href": "chap1.html#understanding-data-management",
    "title": "data_management",
    "section": "Understanding Data Management",
    "text": "Understanding Data Management\n\nWhy Manage Data?\n\nData management supports long-term preservation, access, and use of data\nActivities of data management include planning, documenting, formatting, storing, anonymizing, and controlling access to data\nManaging data helps researchers optimize the use of data, collaborate with other researchers, and answer additional questions in the future\nData management is important for responsible research conduct and meeting requirements from funding agencies, journal publishers, and research institutions\nData management sustains the value of data and increases transparency in research projects\n\n\n\nData Management Stakeholders\n\nThe research lifecycle from the Data Documentation Initiative (DDI) provides a framework for understanding where specific data management practices fall.\nDuring the discovery and planning phase, researchers need to determine what type and format of data they are going to collect, consider ethical issues, and identify potential reusers of the project data. They should also determine the possible costs surrounding data management and identify appropriate data repositories.\nDuring the data collection phase, researchers should follow data management best practices, including file organization, backup and storage strategies, and quality assurance protocols. They should also consider access controls and data security.\nDuring the preparation and data analysis phase, researchers may need to clean, manipulate or process the raw data, and should document any changes to the raw data and create a master version to be analyzed and eventually archived. They should also document analysis procedures, such as modifications to the data, the model used, the code used to run the analysis, and hardware and software specifications.\nDuring the publication and sharing phase, researchers should prepare their data files and other research materials necessary to interpret and reuse the data in the future. They should consult with information professionals or data repository staff and ensure that their data management meets all of the needs and requirements of the repository.\nTrusted repositories perform functions to ensure the long-term management of data, including ensuring the integrity of the data, protecting against data loss, and providing access to data.\n\n\n\nData Management Across the Research Lifecycle\n\nPrimary researchers or principal investigators design the study, specify what data will be collected, and determine how to analyze the data and draw conclusions.\nGraduate students or other staff members collect and manage the data and often analyze it as well.\nInstitutions such as universities and research institutes set internal data management policies, provide data management resources and training, and offer services for archiving data.\nData repositories curate, preserve, and provide access to data, work with data creators to ensure data remain useful over time, and place access restrictions on data when necessary.\nSecondary users of the data include students, faculty, research project team members, private entities, businesses, government officials, journalists, or citizen scientists.\nFunders provide money to support research projects and require researchers to actively and properly manage their data throughout the research project’s life cycle, with an end goal of making the data available and accessible for sharing.\nData management plans are now required of most project proposals, outlining how data will be effectively collected, managed, stored, and preserved for future access.\nPublishers and journals disseminate scientific discovery and maintain the integrity of the scientific record, increasingly encouraging researchers to cite data and issuing policies that require researchers to make data underlying published results available within a data repository.\nProper data management throughout the research lifecycle requires communication and cooperation between various stakeholders."
  },
  {
    "objectID": "chap3.html",
    "href": "chap3.html",
    "title": "data_management",
    "section": "",
    "text": "The module introduces the concept of data organization and good file management practices.\nData organization is important because as the research project progresses, a large volume of data is accumulated, and it can be difficult to find specific data files if they are named inaccurately or inconsistently.\nGood file management practices help identify, locate, and use data effectively, and file naming conventions are important when sharing data with collaborators.\nResearch data files and folders need to be labeled and organized in a systematic way to be identifiable and accessible for current and future users.\nConsistent data filing labeling has numerous benefits, including distinguishable data files, easier browsing and retrieval, logical sorting, and prevention of accidental deletion or overwrite.\nGood data file naming prevents confusion when multiple people are working on shared files.\n\n\n\n\n\nThree main criteria to consider when naming research data files:\n\norganization,\ncontext, and\nconsistency.\n\nCommon elements to consider when developing a file naming strategy include version number, date of creation, creator’s name, content description, team or department name, publication date, and project number.\nFile naming policy should be scalable and avoid generic names to prevent conflicts.\nFile names should be kept short, relevant, and consistent in format, and should not use special characters.\nBatch renaming software can be used to manage large numbers of files and automate consistent naming conventions.\nBulk renaming tools are available for different operating systems, and can be useful in situations such as assigning sequential numbers, using default names, or transferring files between systems with different naming conventions.\n\n\n\n\n\nDistinguish between different versions of data files consistently\nPick a clear versioning method (e.g. using ordinal numbers and decimals)\nAvoid confusing labels such as revision, final, final2, or definitive copy\nRecord changes to data files, even small ones, using auto backup or tracking facilities\nUse version control software such as Subversion and TortoiseSVN for software code\nDelete or discard obsolete versions of data files while retaining original copies\nFurther resources are available for additional information\nMove on to the data file formats module as the next step."
  },
  {
    "objectID": "chap3.html#file-formats-and-transformations",
    "href": "chap3.html#file-formats-and-transformations",
    "title": "data_management",
    "section": "File Formats and Transformations",
    "text": "File Formats and Transformations\n\nFile Formats\n\nThis module covers file formats, compression, data normalization, and transformations.\nFile formats encode information in a computer file, and software needs to recognize that format to access the content within it.\nThe file format is indicated by an extension in the file name, and files in proprietary formats may require specific software to open them, while open formats can be opened by multiple applications.\nFile types are based on text or binary encoding, and creating or saving data in a text format makes the file human-readable and able to be opened in any operating system.\nOpen, non-proprietary, and widely used file formats are less likely to become obsolete and more likely to be readable well into the future.\nConverting or migrating data files from one format to another may be necessary, and checksum algorithm tools can be used to compare file bits and ensure data integrity.\nCompression involves encoding information in fewer bits than the original representation and can result in lossy or lossless compression.\nZip is a de facto standard lossless compression format used on multiple platforms, while tar files are commonly used in Unix or Linux to bundle multiple files.\n\n\n\nData Transformations\n\nData transformations can be done for various reasons during or after a project\nData transformations involve changing the actual data\nAnonymization is an example of a data transformation that can be used in survey data\nQualitative data can be transformed into quantitative data using coding techniques\nData can be transformed to visualize it more effectively, such as converting ratios to percentages for display on charts\nConfidential or sensitive data can be transformed using aggregation or anonymization\nFurther reading resources are available for file formats, compression, normalization, and data transformations\nThe next module on documentation and data citation is recommended"
  },
  {
    "objectID": "chap3.html#documentation-and-data-citation",
    "href": "chap3.html#documentation-and-data-citation",
    "title": "data_management",
    "section": "Documentation and Data Citation",
    "text": "Documentation and Data Citation\n\nDocumentation\n\nThis module covers documentation and data citation.\nDocumentation is important for both the creator and other users to understand the data.\nExamples of data documentation include lab notebooks, codebooks, and methodology reports.\nThere are three levels of data documentation: project level, file/database level, and variable/item level.\nProper data citation is important for the credibility and accessibility of research findings.\nData citation should include information on how to access the underlying data.\n\n\n\nData Citation\n\nThe Joint Declaration on Data Citation Principles was issued in 2014 by Force11.\nCiting data helps identify and acknowledge it, promote reproducibility, track usage and impact, and recognize and reward data creators.\nDataCite recommends five minimum citation elements: creator, year of publication, title, publisher, and identifier.\nAdditional elements that may be added are Version and ResourceType.\nThe UK data service recommends using a title that indicates subject matter, geography, and time period.\nWhen citing data, adopt the same style and order of references as your other works, provide more information than less, and include a date of download for dynamic databases.\nGood practice in data documentation and citation contributes to reproducibility of research.\nEven if the data are unpublished, citation principles still apply.\nFurther reading and a next module on storage and security are available."
  },
  {
    "objectID": "chap3.html#storage-and-security",
    "href": "chap3.html#storage-and-security",
    "title": "data_management",
    "section": "Storage and Security",
    "text": "Storage and Security\n\nStorage\n\nThis module focuses on storing, securing, and backing up research data.\nLosing data can have serious consequences, and hard disk drive crashes are the most common cause of data loss.\nIt’s important to store and back up data securely from the outset, and to plan for storage needs and data management costs.\nNetwork drives are highly recommended as they provide a single copy of data that is backed up regularly and held securely.\nPCs, laptops, and external storage devices can also be used but should not be used as the master copy of data.\nCDs, DVDs, and magnetic tapes degrade over time, and errors writing to them are common, so high-quality products from good manufacturers should be used and periodically refreshed.\n\n\n\nBackup\n\nRegular backups are essential for data management to prevent loss due to hard drive failure or accidental deletion.\nThe 3-2-1 principle of backup involves having three copies of files on at least two different media, with one copy stored offsite.\nRegular testing of backups is crucial to ensure they can be restored if needed.\nSeveral questions should be considered when creating a backup strategy, such as how to back up data, how often to back up data, whether to use incremental or full backups, and how to keep track of different versions of data.\nVarious cloud services are available for data backup, including Dropbox, Google Drive, and OneDrive.\nAdvantages of cloud services for backup include no user intervention required, remote offsite backup, encryption and versioning, and multi-platform support.\nRisks of using cloud services for backup include data stored outside the European economic area, slow data restoration, unencrypted data, and service provider bankruptcy.\n\n\n\nData security\n\nData security means keeping your research data safe from damage, theft, breach of confidentiality, and premature release.\nConsider who needs access to the data and how to enforce permissions and restrictions.\nHave a clear policy on who can make copies of the data and store them on mobile devices.\nInstall up-to-date antivirus software and consider physical security for highly sensitive data.\nUse strong user names and passwords, avoid obvious phrases, don’t write them down, and don’t use the same password for multiple accounts.\nAvoid signing into secure sites from untrusted computers or networks.\n\n\n\nEncryption\n\nEncryption is the process of converting data into an unreadable code that requires an encryption key or password to be accessed.\nEncryption protects data from disclosure in case of loss or theft of a laptop or storage device.\nMedium or high-risk personal or business information must be encrypted if it leaves the university environment.\nA strong encryption password is necessary for data protection, and a reliable backup procedure is essential for password management.\nDifferent encryption software packages are available, and IT support can advise on encrypted flash drive purchases.\nFile deletion is not enough to remove sensitive data from a computer, and three main options for permanent data removal include data erasure, degaussing, and physical destruction.\nFurther reading and resources are available for storage, backup, and security."
  },
  {
    "objectID": "chap4.html",
    "href": "chap4.html",
    "title": "data_management",
    "section": "",
    "text": "FINRA expects data sharing and efficient data management practices\nMetadata standards, documentation, and quality assurance are important in data sharing\nBenefits of data sharing:\n\nReinforces open scientific inquiry\nSupports verification and replication of original results\nPromotes new research and testing of alternative methods\nEncourages collaboration and multiple perspectives\nProvides important teaching resources\nReduces costs by avoiding duplicate data collection efforts\nProtects against faulty or fraudulent data\nEnhances visibility and overall impact of research projects\nPreserves data for future use\nHelps the broader community and individual researchers do better research\n\nKey players in data sharing are the data creator/producer, secondary data user, and data repository\nData repository plays a key role in enhancing discovery and reuse of data and creating formal data citation\nSharing data is encouraged by funders and required in some cases\nData sharing benefits both the broader research community and individual researchers.\n\n\n\n\nChallenges to data sharing:\n\nMaking data shareable takes time and effort\nPerceived risks from loss of control of the data\nData contained confidential or sensitive information\nOwnership of the data may be unclear or problematic\nLack of incentives for sharing data\n\nAdditional challenge: lack of experience and knowledge of data management\nResearchers and information professionals can overcome these challenges by applying data management practices.\n\n\n\n\nLack of career incentives is an obstacle to sharing data.\nData citation provides a standardized method for citing data, and can be used to reward researchers for sharing their data.\nThe Joint Declaration of Data Citation Principles establishes principles for data citation, including credit and attribution, evidence, unique identification, access, persistence, specificity and verifiability, and interoperability and flexibility.\nDataCite is a group that works with data repositories to assign persistent identifiers such as DOIs to data, supporting simple and effective methods of data citation, discovery, and access.\nProper citation of datasets supports reproducibility of research, ensures proper credit for researchers, and enables tracking of data reuse.\nResearchers should consider whether the repository they choose supports the creation of unique data citations that embody the Joint Declaration of Data Citation Principles."
  },
  {
    "objectID": "chap4.html#enabling-sharing",
    "href": "chap4.html#enabling-sharing",
    "title": "data_management",
    "section": "Enabling Sharing",
    "text": "Enabling Sharing\n\nProtecting Confidentiality (Part 1)\n\nResearchers have an ethical obligation to protect the privacy of study participants when collecting data that deals with human subjects.\nConfidentiality breaches can have serious consequences, including negative impacts on a researcher’s career and institution, as well as legal sanctions.\nThere are various types of information that need to be protected, including Personally Identifiable Information (PII), Protected Health Information (PHI), and sensitive information.\nDirect identifiers, such as names and Social Security numbers, are explicit pointers to an individual’s identity and are easy to spot.\nIndirect identifiers, such as race and income, can make unique cases visible when combined with other individual attributes.\nGeographic identifiers, such as Zip Codes, can also be indirect identifiers.\nProtecting confidentiality requires careful consideration and special handling of not only direct identifiers, but also indirect identifiers.\n\n\n\nProtecting Confidentiality (Part 2)\n\nDifferent countries have different laws related to the handling of confidential information and researchers should familiarize themselves with all applicable laws prior to collecting confidential information.\nThe three main laws that affect confidentiality in the United States are the Common Rule, FERPA, and HIPAA.\nThe Common Rule governs human subjects research that is conducted or supported by any federal department or agency and establishes institutional review boards (IRBs) as the entities that oversee the ethical treatment of human subjects in research.\nFERPA applies to educational data and establishes a student’s right of privacy for their educational records, including personally identifiable information (PII).\nHIPAA Privacy Rule protects PHI held or transmitted by a covered entity and provides two main methods for de-identification: expert determination method and safe harbor method.\nICPSR has information on methods for protecting confidentiality, including completely removing problematic variables, top-coding, collapsing or combining, sampling, swapping, and disturbing the data.\nAnonymizing qualitative data is best done using a pre-planned anonymization scheme that modifies the qualitative dataset to protect respondent confidentiality.\nThere are other strategies currently being developed to protect confidential data, but the most common strategy remains to anonymize data by removing variables, applying statistical techniques, or redacting information to lessen the potential for identifying an individual.\nResearchers should consider future research questions and carefully consider a variable’s analytic importance to determine the best strategy for anonymizing data to maximize usability, and may consult their institutional review board, statistical experts, or information professionals.\n\n\n\nIntellectual Property and Data Ownership\n\nData ownership and intellectual property rights can complicate sharing research data.\nIntellectual property rights apply to any work created or invented with intellectual effort.\nDifferent forms of research data can have different intellectual property rights and legal jurisdictions.\nResearchers should resolve any data ownership issues before sharing data.\nData ownership can be complicated due to multiple stakeholders and collaborations.\nResearchers should come to an agreement on data usage and ownership at the beginning of a project.\nInstitutional policies and funder policies can influence data ownership and sharing.\nResearchers may need permission from data producers to share proprietary data.\nInformation and legal professionals can assist researchers in determining policies that affect data ownership.\n\n\n\nAccess\n\nSharing research data has benefits and challenges.\nAccess to data may vary based on external limitations or researchers’ needs.\nData ownership issues or data containing sensitive or confidential information may affect how and where a researcher provides access.\nThree types of restrictions that are commonly placed on data: embargos, technological access restrictions, and data use agreements.\nEmbargos are a specified period of time when access to data will be restricted.\nTechnological access restrictions may require users to log in to a particular system and authenticate the relationship to an institution to access certain data.\nData use agreements explicitly outline an agreement between the data producer and secondary data user.\nIdeally, researchers should make data as open as possible, but this may not always be feasible.\nApplying a standard Creative Commons license can positively impact the potential for reuse.\nCreative Commons provides a robust legal code, a human-readable summary, and a machine-readable layer of code that can help make resources interoperable across systems.\nFive main Creative Commons license categories: Attribution, NonCommercial, No Derivative Works, ShareAlike, and CC0.\nResearchers should explain restrictions within the terms of use and apply a standard license to enable informed reuse."
  },
  {
    "objectID": "chap5.html",
    "href": "chap5.html",
    "title": "data_management",
    "section": "",
    "text": "Archiving digital data is crucial for long-term accessibility and usability.\nTrustworthy repositories can provide strategies to preserve data authenticity and integrity.\nUnderstanding preservation needs, authenticity, integrity, and metadata is important.\nDigital data are at risk due to benign neglect, bit rot, obsolescence, and insufficient documentation.\nBit rot is the degradation or corruption of machine-readable information over time.\nObsolescence occurs when hardware or software becomes outdated and data becomes trapped.\nInsufficient documentation makes it impossible to interpret data in the future.\nComplete and appropriate documentation and metadata are essential for long-term data preservation.\n\n\n\n\n\nPreservation of digital content aims to ensure authenticity and integrity\nAuthenticity means the data is genuine and free from tampering\nEstablishing authenticity requires both researchers and data repositories to have procedures and documentation in place\nBest practices for maintaining authenticity include maintaining a single master file, regulating write access, recording all changes, and archiving copies of master files\nIntegrity means the digital object has not been corrupted over time or in transit between storage locations or systems\nBest practices for maintaining integrity include backing up critical files, storing master files in open source formats, verifying backup copies, storing copies on two types of storage media, and copying/migrating files to new storage media every 2-5 years\nTechnology has increased our ability to collect and analyze data but also has vulnerabilities, requiring an active preservation strategy based on standards and best practices.\n\n\n\n\n\nMetadata is structured information that describes, explains, locates, or represents something else and is necessary for resource discovery, organization, interoperability, identification, and preservation.\nThere are three types of metadata: descriptive, administrative, and structural.\nDescriptive metadata is used for discovery and identification, including information such as title, author, and abstract.\nAdministrative metadata is structured information regarding the management and tracking of data over time and can be rights management or preservation metadata.\nStructural metadata describes the physical or logical structure of digital objects.\nMetadata standards often require adherence to specific representation rules and controlled vocabularies to ensure consistency in data entry and authoritative use of terms.\nGood standardized metadata allows for interoperability across systems, data structures, and interfaces, and facilitates the efficient dissemination of digital materials.\nThe open archive initiative protocol for metadata harvesting (OAI-PMH) is an example of how standardized metadata can be shared across distributed systems."
  },
  {
    "objectID": "chap5.html#trustworthy-repositories",
    "href": "chap5.html#trustworthy-repositories",
    "title": "data_management",
    "section": "Trustworthy Repositories",
    "text": "Trustworthy Repositories\n\nDemonstrating Trustworthiness\n\nTrustworthy repositories are crucial for the long-term preservation of data.\nData repositories oversee the long-term storage and preservation of data and come in different types, including domain, institutional, and location-specific.\nIt’s important to talk to repository staff early in the research data life cycle to understand specific data management requirements.\nTo ensure the trustworthiness of a repository, they must demonstrate compliance with standards and best practices through transparent policies and seek audit and certification.\nAdhering to standards and best practices is essential to achieving trustworthiness.\nTransparent policies and procedures are necessary for repositories to demonstrate their trustworthiness, and they should make their archival and digital preservation policies readily available to stakeholders.\nThe Data Seal of Approval, DRAMBORA, and ISO 16363 Audit and Certification of Trustworthy Digital Repositories are examples of audit schemes used to assess the trustworthiness of digital repositories based on adherence, standards, and best practices.\n\n\n\nData Curation Standards and Best Practices (Part 1)\n\nISO 14721 is a reference model for an open archival information system (OAIS).\nOAIS defines elements and processes within digital repositories, and establishes responsibilities for long-term preservation of digital information.\nRepositories that consider themselves to be OAIS archives are presumed to have mechanisms and workflows in place to properly safeguard digital materials for a designated community.\nOAIS model has six functional entities: Ingest, Archival Storage, Data Management, Administration, Preservation, and Access.\nIngest function performs several tasks to establish evidence of authenticity, ensure files are in proper formats, and normalize files to formats optimal for long-term preservation.\nStandard ingest procedures include consideration of sustainability and use factors that affect feasibility and cost of long-term file preservation.\nPreservation optimization presumes that supplementary documentation is included with the file to enable appropriate interpretation and use of the data.\nOnce ingest tasks have been completed, the SIP becomes an AIP which is managed by the archival storage function.\nData management function provides functionality for generating, maintaining, and accessing metadata to document files housed in archival storage.\nAccess function coordinates requests and delivers the final dissemination information package (DIP) to users who can retrieve data files.\n\n\n\nData Curation Standards and Best Practices (Part 2)\n\nPreservation planning function provides recommendations for preservation and planning strategies to ensure that data are accessible and understandable to users over time.\nMigration and emulation are common preservation strategies to protect against data loss due to obsolescence.\nPreservation planning involves ongoing evaluation of archival materials to identify file migration requirements, recommend changes to archive processes and policies, and report any risks.\nAdministration function handles data submission agreements with data producers, establishes quality standards, and manages infrastructure configurations.\nTrustworthy repositories have short-term and long-term business plans for sustainability with transparent accounting practices, and they analyze and document financial risks, investments and expenditures.\nStoring copies of digital content in geographically distributed locations and collaborative partnerships can help protect against physical losses from disasters or organizational failure.\nEconomics sustainability for long term preservation is an ongoing concern in the field of data archiving because the cost of long term preservation is still largely unclear.\nTrustworthy data repositories make a verifiable commitment to archiving data and expend significant amounts of labor, funding, research and assessment to safeguard data."
  }
]